{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph RAG Querying\n",
        "\n",
        "This notebook demonstrates and compares two complementary approaches to Retrieval Augmented Generation (RAG) for Building Information Modeling (BIM) data, building upon the data processed and embedded in Part 1 (`01_data_integration.ipynb`) [![Open data integration notebook in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/qaecy/built2025/blob/main/notebooks/01_data_integration.ipynb).\n",
        "\n",
        "**Goal:** Query the processed data (RDF graph and text embeddings) using two different RAG methods.\n",
        "\n",
        "**The Two Approaches:**\n",
        "1.  **Vector-Based RAG**: Uses semantic search over pre-computed text embeddings of building entities.\n",
        "   - *Best for:* Finding relevant text passages, understanding entity types and descriptions (e.g., \"What types of doors are in the building?\").\n",
        "   - *Analogy:* Think of this as a smart search engine that finds relevant text and uses it to answer questions.\n",
        "2.  **Query-Based RAG**: Translates natural language questions into SPARQL queries executed against an RDF knowledge graph.\n",
        "   - *Best for:* Precise property lookups, relationship queries, counting instances (e.g., \"What is the area of room A103?\").\n",
        "   - *Analogy:* Think of this as a database query system that can precisely look up properties and relationships.\n",
        "\n",
        "**When to use each**:\n",
        "- Use Vector RAG for descriptive, similarity-based questions.\n",
        "- Use Graph RAG for precise, property-based questions.\n",
        "- Consider combining both for complex queries.\n",
        "\n",
        "**Learning Objectives**\n",
        "By the end of this notebook, you will:\n",
        "1. Understand when to use vector-based vs. query-based RAG.\n",
        "2. Set up and use both RAG approaches with BIM data.\n",
        "3. Compare the strengths and limitations of each method.\n",
        "4. See how the approaches can be combined for more powerful querying.\n",
        "\n",
        "**Notebook Structure**\n",
        "1. **Setup and Dependencies** - Configure the environment and load necessary libraries.\n",
        "2. **Method 1: Vector-Based RAG** - Implement semantic search over text embeddings.\n",
        "3. **Method 2: Graph-Based RAG** - Query the knowledge graph using SPARQL.\n",
        "4. **Comparison, Summary, and Next Steps** - Analyze the approaches and explore extensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "This notebook can run in either Google Colab or locally. The setup cell below automatically configures your environment by detecting whether it's running in Colab or locally, cloning the repository if needed (Colab), and installing dependencies from `requirements.txt`.\n",
        "\n",
        "**Key Point:** This setup ensures the notebook runs consistently anywhere with minimal configuration. It is identical to Part 1's setup.\n",
        "\n",
        "**Key Dependencies and Their Purpose**\n",
        "- **LangChain**: Framework for building RAG applications.\n",
        "- **pyoxigraph**: Graph database for executing SPARQL queries.\n",
        "- **FAISS**: Vector similarity search library.\n",
        "- **OpenAI API**: Required for both RAG approaches (different models used).\n",
        "  - Vector RAG uses `gpt-3.5-turbo` (more cost-effective for text generation).\n",
        "  - Graph RAG uses `gpt-4o` (better at SPARQL generation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    from IPython import get_ipython\n",
        "    IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Configure environment\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/qaecy/bilt2025.git\n",
        "    %cd bilt2025\n",
        "    requirements_path = \"requirements.txt\"\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "else:    \n",
        "    # Find requirements.txt based on current directory\n",
        "    current_dir = Path().resolve()\n",
        "    requirements_path = \"../requirements.txt\" if current_dir.name == \"notebooks\" else \"requirements.txt\"\n",
        "    print(f\"Looking for requirements at: {Path(requirements_path).resolve()}\")\n",
        "\n",
        "# Install dependencies if requirements.txt exists\n",
        "if os.path.exists(requirements_path):\n",
        "    %pip install -r {requirements_path}\n",
        "    if IN_COLAB:\n",
        "        %pip install -e .\n",
        "    print(\"✓ Environment setup complete\")\n",
        "else:\n",
        "    print(\"⚠️ Could not find requirements.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup Paths\n",
        "\n",
        "Here we import the necessary libraries and set up paths to the data prepared in the previous lab. Note the different handling for Colab vs. local environments to ensure correct path setup.\n",
        "\n",
        "**Key Point:** These libraries provide the tools for our RAG approaches, including our custom RAG classes (`VectorRAG` and `QueryRAG`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add project to path if running locally\n",
        "if not IN_COLAB:\n",
        "    project_root = Path().resolve()\n",
        "    if project_root.name == 'notebooks':\n",
        "        project_root = project_root.parent\n",
        "    if str(project_root) not in sys.path:\n",
        "        sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import our simplified RAG implementations\n",
        "from src.graph_rag.vector_based import VectorRAG\n",
        "from src.graph_rag.query_based import QueryRAG\n",
        "\n",
        "# Define paths\n",
        "project_root = Path().resolve()\n",
        "if project_root.name == 'notebooks':\n",
        "    project_root = project_root.parent\n",
        "\n",
        "DATA_DIR = project_root / \"data\"\n",
        "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\" / \"buildingsmart_duplex\"\n",
        "GRAPH_DIR = DATA_DIR / \"graph\" / \"buildingsmart_duplex\"\n",
        "SCHEMA_FILE = DATA_DIR / \"graph\" / \"reduced_schema.txt\"\n",
        "EXAMPLES_FILE = DATA_DIR / \"graph\" / \"few_shot_examples.json\"\n",
        "\n",
        "print(f\"Embeddings directory: {EMBEDDINGS_DIR}\")\n",
        "print(f\"Graph directory: {GRAPH_DIR}\")\n",
        "print(f\"Schema file: {SCHEMA_FILE}\")\n",
        "print(f\"Examples file: {EXAMPLES_FILE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Method 1: Vector-Based RAG\n",
        "\n",
        "In this section, we'll demonstrate a vector-based RAG approach using LangChain. This approach is particularly useful for:\n",
        "- Finding similar text descriptions\n",
        "- Understanding entity types and their characteristics\n",
        "- Answering questions that require semantic understanding\n",
        "\n",
        "**Key Point:** This approach leverages pre-computed embeddings for semantic search and answer generation, acting like a smart search engine for relevant text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Explore Available Embeddings\n",
        "\n",
        "Let's start by listing the embedding files created in Part 1. These JSON files contain the text representations and vector embeddings for entities from our building models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display available embedding files\n",
        "embedding_files = list(EMBEDDINGS_DIR.glob(\"*.json\"))\n",
        "embedding_info = [{\n",
        "    \"Filename\": file.name,\n",
        "    \"Source File\": file.stem.replace(\"_embeddings\", \".ttl\"),\n",
        "    \"Size (MB)\": round(file.stat().st_size / (1024 * 1024), 2)\n",
        "} for file in embedding_files]\n",
        "\n",
        "display(pd.DataFrame(embedding_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Initialize Vector RAG System\n",
        "\n",
        "We initialize our `VectorRAG` class from `src.graph_rag.vector_based.py`.\n",
        "\n",
        "**Behind the Scenes (`VectorRAG`):**\n",
        "1. **Document Loading**: Loads pre-computed *embeddings* (text + vector) from the specified JSON files.\n",
        "2. **Vector Store Creation**: Embeddings are loaded into a FAISS vector store for efficient *semantic similarity search*.\n",
        "3. **Query Processing**: The input question is embedded, and FAISS finds the most semantically similar documents (text chunks) based on vector distance.\n",
        "4. **Context Retrieval & Prompting**: The retrieved text chunks serve as context. An internal LangChain prompt template (`RetrievalQA`) combines this context with the user's original question.\n",
        "5. **Answer Generation**: The combined prompt (context + question) is sent to an OpenAI model (`gpt-3.5-turbo` by default) to synthesize a natural language answer.\n",
        "6. **Source Tracking**: The system keeps track of the *source documents* (text chunks) used to generate the answer, providing transparency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize our vector based RAG\n",
        "vector_rag = VectorRAG(embedding_files=list(EMBEDDINGS_DIR.glob(\"*.json\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Ask Questions (Vector-Based)\n",
        "\n",
        "Now, let's ask some questions suited for this approach. The `vector_rag.query(question, top_k=k)` method works as follows:\n",
        "1. Embeds the input `question`.\n",
        "2. Performs a similarity search in the FAISS vector store to find the `top_k` most relevant text chunks (documents/context).\n",
        "3. Sends the retrieved context and the original question to the LLM (via the prompt template) to generate an answer.\n",
        "4. Returns the generated answer along with the source documents used for transparency.\n",
        "\n",
        "**Key Point:** The system finds relevant text snippets and uses an LLM to synthesize an answer. The sources show *which* text chunks were used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of questions to ask\n",
        "questions = [\n",
        "    \"What types of doors are in the building?\",\n",
        "    \"How many windows are in the building?\",\n",
        "    \"What materials are used in the exterior walls?\",\n",
        "    \"What rooms have smoke detectors?\"\n",
        "]\n",
        "\n",
        "# Ask each question and display results\n",
        "top_k = 5\n",
        "for question in questions:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Get answer from RAG system\n",
        "    result = vector_rag.query(question, top_k=top_k)\n",
        "    \n",
        "    # Display the answer\n",
        "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
        "    \n",
        "    # Display sources in a table\n",
        "    sources_df = pd.DataFrame([{\n",
        "        'Entity': s['entity'],\n",
        "        'Source': s['source'],\n",
        "        'Context': s['text'][:100] + '...'  # Truncate long context\n",
        "    } for s in result['sources']])\n",
        "    \n",
        "    print(\"\\nSources:\")\n",
        "    display(sources_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Method 2: Graph-Based RAG (Query-Based)\n",
        "\n",
        "Now, let's explore the query-based RAG approach that interacts directly with the knowledge graph using SPARQL. This method is particularly useful for:\n",
        "- Precise property lookups\n",
        "- Relationship queries\n",
        "- Counting instances\n",
        "- Complex property path queries\n",
        "\n",
        "**Key Point:** This approach is like a database query system that can precisely look up properties and relationships within the structured graph data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Initialize Graph RAG System\n",
        "\n",
        "We initialize our `QueryRAG` class from `src.graph_rag.query_based.py`. This requires the OpenAI API key to be set, as it relies on an LLM (GPT-4o recommended) for natural language to SPARQL translation.\n",
        "\n",
        "**Key Concept: Context is Crucial for LLM Guidance**\n",
        "Unlike the vector approach which relies on embedding similarity, the query-based approach needs explicit guidance for the LLM to translate natural language into precise SPARQL. We provide this guidance through two key context files:\n",
        "1. `reduced_schema.txt`: Contains essential classes and properties the LLM needs to know about.\n",
        "2. `few_shot_examples.json`: Provides concrete examples of question-to-SPARQL translation patterns.\n",
        "These files are manually curated to give the LLM the necessary vocabulary and structural patterns to work with our specific graph data.\n",
        "\n",
        "**Behind the Scenes (`QueryRAG`):**\n",
        "1. **Graph Loading**: Loads *RDF triples* from the specified TTL files into an in-memory `pyoxigraph` graph store.\n",
        "2. **NL-to-SPARQL Context Prep**: Prepares context for the LLM, including:\n",
        "  - The simplified schema description (`reduced_schema.txt`) to provide vocabulary (available classes, properties) and relationship information.\n",
        "  - Few-shot examples (`few_shot_examples.json`) demonstrating common question-to-SPARQL translation patterns. The chosen examples specifically aim to teach the LLM:\n",
        "    - How to **count** instances (`SELECT (COUNT(...) ...)` pattern).\n",
        "    - How to retrieve **labels** along with instances (`SELECT ?instance ?label ... OPTIONAL { ?instance rdfs:label ?label . }` pattern).\n",
        "    - How to perform **property lookups** that require traversing through intermediate PropertySet nodes (`?instance ifc:hasPropertySet ?pset . ?pset prop:PropertyName ?valueNode . ?valueNode rdf:value ?value .` pattern).\n",
        "    - How to check for the **existence** of something (`ASK WHERE {...}` pattern - *Note: While not in the default `few_shot_examples.json`, the system prompt encourages this, and we test it later.*).\n",
        "3. **SPARQL Generation**: Sets up a LangChain chain (`GraphSparqlQAChain`) that sends the natural language question, schema, and examples to the LLM (`gpt-4o` by default) to generate a SPARQL query.\n",
        "4. **Query Execution**: The generated SPARQL query is executed directly against the `pyoxigraph` graph store.\n",
        "5. **Answer Formatting**: The raw results from the SPARQL query are formatted (currently basic formatting, potentially LLM-enhanced in future) into a readable natural language answer.\n",
        "6. **Transparency**: The intermediate *generated SPARQL query* and the *raw graph results* are returned alongside the final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if OPENAI_API_KEY is set (required for QueryRAG)\n",
        "api_key_set = \"OPENAI_API_KEY\" in os.environ\n",
        "if not api_key_set:\n",
        "    print(\"\\n⚠️ WARNING: OPENAI_API_KEY environment variable not set.\")\n",
        "    print(\"QueryRAG requires an OpenAI API key to function.\")\n",
        "    print(\"Please set it (e.g., os.environ['OPENAI_API_KEY'] = 'your_key') or the next cell will fail.\")\n",
        "\n",
        "# Initialize QueryRAG (only if API key is set)\n",
        "query_rag = None\n",
        "if api_key_set:\n",
        "    ttl_files = list(GRAPH_DIR.glob(\"*.ttl\"))\n",
        "    if not ttl_files:\n",
        "        print(f\"Error: No TTL files found in {GRAPH_DIR}\")\n",
        "    elif not SCHEMA_FILE.is_file():\n",
        "        print(f\"Error: Schema file not found at {SCHEMA_FILE}\")\n",
        "    elif not EXAMPLES_FILE.is_file():\n",
        "        print(f\"Error: Examples file not found at {EXAMPLES_FILE}\")\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"\\nInitializing QueryRAG with {len(ttl_files)} TTL files...\")\n",
        "            query_rag = QueryRAG(\n",
        "                ttl_files=ttl_files,\n",
        "                schema_file=SCHEMA_FILE,\n",
        "                examples_file=EXAMPLES_FILE\n",
        "                # llm_model=\"gpt-4o\" # Default is now gpt-4o\n",
        "            )\n",
        "            print(\"✓ QueryRAG initialized.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error initializing QueryRAG: {e}\")\n",
        "else:\n",
        "     print(\"\\nSkipping QueryRAG initialization due to missing API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Ask Questions (Graph-Based)\n",
        "\n",
        "Let's ask questions that are better suited for graph traversal and precise lookups. The `query_rag.query(question)` method works as follows:\n",
        "1. Sends the `question`, schema information, and few-shot examples to the LLM.\n",
        "2. The LLM generates a SPARQL query based on its understanding of the question and the provided context.\n",
        "3. The generated SPARQL query is executed against the loaded `pyoxigraph` graph database.\n",
        "4. The raw results from the query are processed (potentially using the LLM again for summarization, though currently basic formatting is applied) to generate a natural language answer.\n",
        "5. Returns the final answer, the generated SPARQL query, and the raw graph results for transparency.\n",
        "\n",
        "**Key Point:** This method provides precise answers derived directly from graph queries, offering high accuracy for structured data retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define example questions for QueryRAG\n",
        "graph_questions = [\n",
        "    \"How many IfcWindow instances are there?\", # Similar COUNT pattern, different class\n",
        "    \"Show me the labels of all IfcSpace instances.\", # Similar SELECT+LABEL pattern, different class\n",
        "    \"What is the volume of the space labeled 'A101'?\", # Similar property lookup pattern, different property/instance\n",
        "    \"Does a door with label 'M_Single-Flush:0915 x 2134mm:190721' exist?\" # Similar ASK pattern, different instance/label\n",
        "]\n",
        "\n",
        "# Query using QueryRAG if initialized\n",
        "if query_rag and query_rag.chain:\n",
        "    for question in graph_questions:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        try:\n",
        "            result = query_rag.query(question)\n",
        "            \n",
        "            print(\"\\nGenerated SPARQL:\")\n",
        "            print(result[\"sparql_query\"] or \"N/A\")\n",
        "            \n",
        "            # Display Raw Results (nicer formatting)\n",
        "            print(\"\\nRaw Results:\")\n",
        "            if isinstance(result[\"raw_results\"] , list):\n",
        "                if not result[\"raw_results\"]:\n",
        "                    print(\"[]\")\n",
        "                else:\n",
        "                    print(\"[\")\n",
        "                    for i, res_dict in enumerate(result[\"raw_results\"]):\n",
        "                        if i >= 5: # Limit display for brevity\n",
        "                             print(f\"  ... ({len(result['raw_results']) - 5} more)\")\n",
        "                             break\n",
        "                        print(f\"  {res_dict}\")\n",
        "                    print(\"]\")\n",
        "            else:\n",
        "                print(result[\"raw_results\"] or \"N/A\")\n",
        "            \n",
        "            print(\"\\nFormatted Answer:\")\n",
        "            print(result[\"answer\"])\n",
        "            \n",
        "        except Exception as e:\n",
        "             print(f\"\\n❌ An unexpected error occurred during query processing: {e}\")\n",
        "else:\n",
        "    print(\"QueryRAG was not initialized successfully (check API key and file paths), skipping queries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Comparison, Summary, and Next Steps\n",
        "\n",
        "We've explored two distinct RAG approaches for BIM data based on the results from our queries. Here's a summary reflecting their observed strengths and ideal use cases:\n",
        "\n",
        "### Comparison: When to Use Each Approach (Based on Observed Results)\n",
        "\n",
        "| Use Case                     | Vector RAG                                    | Graph RAG                                         | Notes Based on Experiments                                          |\n",
        "|------------------------------|-----------------------------------------------|---------------------------------------------------|---------------------------------------------------------------------|\n",
        "| Finding similar descriptions | ✅ Good (e.g., Wall Materials, Smoke Detectors) | ❌ Not ideal                                      | Vector RAG excels at finding relevant *text* passages.              |\n",
        "| Precise property lookups     | ⚠️ Limited (If explicitly in text)           | ✅ **Best** (e.g., Volume of Space A101)            | Graph queries retrieve exact values; revealed multiple Volume entries. |\n",
        "| Relationship queries         | ⚠️ Implicit                                  | ✅ **Best** (Built for structure)                  | Graph explicitly models links (e.g., Space -> PSet -> Volume).      |\n",
        "| Counting instances           | ⚠️ Approximate/Inaccurate (Window count wrong) | ✅ **Precise** (Window count correct)             | Graph `COUNT` is reliable; Vector RAG seemed to misinterpret.         |\n",
        "| Complex property paths       | ❌ Impossible                                | ✅ **Best** (e.g., PSet traversal for properties) | SPARQL handles multi-step traversals required for properties.     |\n",
        "| General knowledge / Fuzzy Qs | ✅ Good (e.g., Smoke detector location)        | ⚠️ Limited (Needs specific schema/examples)       | Vector search handles broader topics; Graph needs precise mapping.    |\n",
        "| Exact data point retrieval   | ⚠️ Depends on text source                    | ✅ **Best** (e.g., Count, Volume, Existence check) | Graph ensures accuracy for structured facts via SPARQL.               |\n",
        "\n",
        "**Key Takeaway:** Neither approach is universally \"better\"; they are complementary tools, validated by our results. \n",
        "- **Vector RAG** proved useful for retrieving descriptive text and handling fuzzier queries, but struggled with precise counts or specific property values not explicitly stated in the text.\n",
        "- **Graph RAG** excelled at precise, structured data retrieval (counting, property lookups, existence checks) but requires curated context (schema, examples) and correct query execution logic.\n",
        "\n",
        "### Summary\n",
        "This notebook demonstrated:\n",
        "- Setting up and using Vector-Based RAG, highlighting its strengths in semantic text retrieval (materials, detector info) and weaknesses in precise counting.\n",
        "- Setting up and using Graph-Based RAG, showing its power for accurate data retrieval (counts, labels, properties like Volume) once context (schema/examples) is provided and execution bugs (like the newline issue) are resolved.\n",
        "\n",
        "### Future Directions & Next Steps\n",
        "Potential next steps could include:\n",
        "- **Hybrid Approaches:** Experimenting with workflows that use vector search to find entities and graph search to get details, or using graph context to enrich embeddings.\n",
        "- **Agent Systems:** Building an agent that intelligently routes questions to the appropriate RAG system based on the question type.\n",
        "- **Context Enhancement:** Improving the Graph RAG's `reduced_schema.txt` and `few_shot_examples.json` to handle more query types.\n",
        "- **Visualization:** Integrating tools to display query results spatially or graphically.\n",
        "- **Scaling:** Testing and optimizing these techniques on larger, more complex building models.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
