{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph RAG Querying\n",
        "\n",
        "This notebook demonstrates a simplified Retrieval Augmented Generation (RAG) approach for Building Information Modeling data using LangChain.\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- Set up a simple LangChain-based RAG pipeline\n",
        "- Load pre-computed entity embeddings into a FAISS vector store\n",
        "- Perform semantic search and generate answers using a small LLM\n",
        "- Analyze and visualize the results\n",
        "\n",
        "## 0. Setup\n",
        "This notebook can run in either Google Colab or locally. The setup cell will automatically configure your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    from IPython import get_ipython\n",
        "    IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Configure environment\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/qaecy/built2025.git\n",
        "    %cd built2025\n",
        "    requirements_path = \"requirements.txt\"\n",
        "else:    \n",
        "    # Find requirements.txt based on current directory\n",
        "    current_dir = Path().resolve()\n",
        "    requirements_path = \"../requirements.txt\" if current_dir.name == \"notebooks\" else \"requirements.txt\"\n",
        "    print(f\"Looking for requirements at: {Path(requirements_path).resolve()}\")\n",
        "\n",
        "# Install dependencies if requirements.txt exists\n",
        "if os.path.exists(requirements_path):\n",
        "    %pip install -r {requirements_path}\n",
        "    if IN_COLAB:\n",
        "        %pip install -e .\n",
        "    print(\"✓ Environment setup complete\")\n",
        "else:\n",
        "    print(\"⚠️ Could not find requirements.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup Paths\n",
        "\n",
        "We'll import necessary libraries and set up paths for our data and embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add project to path if running locally\n",
        "if not IN_COLAB:\n",
        "    project_root = Path().resolve()\n",
        "    if project_root.name == 'notebooks':\n",
        "        project_root = project_root.parent\n",
        "    if str(project_root) not in sys.path:\n",
        "        sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import our simplified RAG implementation\n",
        "from src.graph_rag.vector_based import VectorRAG\n",
        "\n",
        "# Define paths\n",
        "project_root = Path().resolve()\n",
        "if project_root.name == 'notebooks':\n",
        "    project_root = project_root.parent\n",
        "\n",
        "DATA_DIR = project_root / \"data\"\n",
        "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\" / \"buildingsmart_duplex\"\n",
        "GRAPH_DIR = DATA_DIR / \"graph\" / \"buildingsmart_duplex\"\n",
        "\n",
        "print(f\"Embeddings directory: {EMBEDDINGS_DIR}\")\n",
        "print(f\"Graph directory: {GRAPH_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Simple RAG with LangChain\n",
        "\n",
        "In this section, we'll demonstrate a simplified LangChain-based RAG approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Explore Available Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display available embedding files\n",
        "embedding_files = list(EMBEDDINGS_DIR.glob(\"*.json\"))\n",
        "embedding_info = [{\n",
        "    \"Filename\": file.name,\n",
        "    \"Source File\": file.stem.replace(\"_embeddings\", \".ttl\"),\n",
        "    \"Size (MB)\": round(file.stat().st_size / (1024 * 1024), 2)\n",
        "} for file in embedding_files]\n",
        "\n",
        "display(pd.DataFrame(embedding_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Initialize RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize our BIM RAG system\n",
        "# Initialize our Vector RAG system\n",
        "# llm_model = \"Qwen/Qwen2.5-Coder-0.5B-Instruct\" # NOTE faster, less accurate\n",
        "# llm_model = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\" # NOTE slower, more accurate\n",
        "# llm_model = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
        "# llm_model = \"Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4\" # NOTE cant run this, require gpu\n",
        "# llm_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "# llm_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "# llm_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# TODO what model to use? every of these models struggle to produce usable answers.\n",
        "#   1) could make even cleaner data for embedding\n",
        "#   2) look into better llm models\n",
        "# files = [\n",
        "#     EMBEDDINGS_DIR / \"Duplex_A_20110907_embeddings.json\",\n",
        "# ]\n",
        "vector_rag = VectorRAG(embedding_files=list(EMBEDDINGS_DIR.glob(\"*.json\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Ask Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of questions to ask\n",
        "questions = [\n",
        "    \"What types of doors are in the building?\",\n",
        "    \"How many windows are in the building?\",\n",
        "    \"What materials are used in the exterior walls?\",\n",
        "    \"What rooms have smoke detectors?\"\n",
        "]\n",
        "\n",
        "# Ask each question and display results\n",
        "top_k = 5\n",
        "for question in questions:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Get answer from RAG system\n",
        "    result = vector_rag.query(question, top_k=top_k)\n",
        "    \n",
        "    # Display the answer\n",
        "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
        "    \n",
        "    # Display sources in a table\n",
        "    sources_df = pd.DataFrame([{\n",
        "        'Entity': s['entity'],\n",
        "        'Source': s['source'],\n",
        "        'Context': s['text'][:100] + '...'  # Truncate long context\n",
        "    } for s in result['sources']])\n",
        "    \n",
        "    print(\"\\nSources:\")\n",
        "    display(sources_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Visualize Sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom question\n",
        "custom_question = \"What are the dimensions of the kitchen?\"\n",
        "\n",
        "# Get answer\n",
        "result = vector_rag.query(custom_question)\n",
        "print(f\"Question: {custom_question}\\n\")\n",
        "print(f\"Answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "# Get analysis\n",
        "if 'analysis' in result:\n",
        "    analysis = result['analysis']\n",
        "    \n",
        "    # Create visualizations\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Source distribution\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sources = list(analysis['source_distribution'].keys())\n",
        "    counts = list(analysis['source_distribution'].values())\n",
        "    plt.bar(sources, counts)\n",
        "    plt.title('Sources Used in Answer')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Number of References')\n",
        "    \n",
        "    # Entity types\n",
        "    if 'entity_types' in analysis and analysis['entity_types']:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        types = list(analysis['entity_types'].keys())\n",
        "        type_counts = list(analysis['entity_types'].values())\n",
        "        plt.bar(types, type_counts)\n",
        "        plt.title('Entity Types Referenced')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('Count')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Graph RAG (Query-Based)\n",
        "\n",
        "Now, let's explore a different RAG approach that queries a knowledge graph directly using SPARQL.\n",
        "This method leverages an LLM to translate natural language questions into SPARQL queries, which are then executed against a graph database loaded from TTL files.\n",
        "\n",
        "**Key components:**\n",
        "- **Graph Store:** `pyoxigraph` is used to load RDF triples from TTL files.\n",
        "- **NL-to-SPARQL:** An LLM (e.g., GPT-4o) translates questions to SPARQL using schema hints and few-shot examples.\n",
        "- **Schema Context:** A boiled-down schema description (`boiled_down_schema.txt`) provides vocabulary.\n",
        "- **Few-Shot Examples:** `few_shot_examples.json` demonstrates query patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the GraphRAG class\n",
        "# Import the QueryRAG class\n",
        "from src.graph_rag.query_based import QueryRAG\n",
        "\n",
        "# Define paths (relative to project root established earlier)\n",
        "TTL_DIR = GRAPH_DIR # Assuming TTLs are in the 'graph/buildingsmart_duplex' directory\n",
        "SCHEMA_FILE = DATA_DIR / \"graph\" / \"reduced_schema.txt\"\n",
        "EXAMPLES_FILE = DATA_DIR / \"graph\" / \"few_shot_examples.json\"\n",
        "\n",
        "print(f\"TTL directory: {TTL_DIR}\")\n",
        "print(f\"Schema file: {SCHEMA_FILE}\")\n",
        "print(f\"Examples file: {EXAMPLES_FILE}\")\n",
        "\n",
        "# Check if OPENAI_API_KEY is set (required for GraphRAG)\n",
        "api_key_set = \"OPENAI_API_KEY\" in os.environ\n",
        "if not api_key_set:\n",
        "    print(\"\\n⚠️ WARNING: OPENAI_API_KEY environment variable not set.\")\n",
        "    print(\"GraphRAG requires an OpenAI API key to function.\")\n",
        "    print(\"Please set it (e.g., os.environ['OPENAI_API_KEY'] = 'your_key') or the next cell will fail.\")\n",
        "\n",
        "# Initialize GraphRAG (only if API key is set)\n",
        "# Initialize QueryRAG (only if API key is set)\n",
        "query_rag = None\n",
        "if api_key_set:\n",
        "    ttl_files = list(TTL_DIR.glob(\"*.ttl\"))\n",
        "    if not ttl_files:\n",
        "        print(f\"Error: No TTL files found in {TTL_DIR}\")\n",
        "    elif not SCHEMA_FILE.is_file():\n",
        "        print(f\"Error: Schema file not found at {SCHEMA_FILE}\")\n",
        "    elif not EXAMPLES_FILE.is_file():\n",
        "        print(f\"Error: Examples file not found at {EXAMPLES_FILE}\")\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"\\nInitializing QueryRAG with {len(ttl_files)} TTL files...\")\n",
        "            query_rag = QueryRAG(\n",
        "                ttl_files=ttl_files,\n",
        "                schema_file=SCHEMA_FILE,\n",
        "                examples_file=EXAMPLES_FILE\n",
        "                # llm_model=\"gpt-4o\" # Default is now gpt-4o\n",
        "            )\n",
        "            print(\"✓ QueryRAG initialized.\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error initializing QueryRAG: {e}\")\n",
        "else:\n",
        "     print(\"\\nSkipping QueryRAG initialization due to missing API key.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define example questions for GraphRAG\n",
        "# Define example questions for QueryRAG\n",
        "graph_questions = [\n",
        "    \"How many IfcDoor instances exist?\", # Uses COUNT pattern\n",
        "    \"Show me the labels of all IfcWall instances.\", # Uses basic select + optional label\n",
        "    \"What is the Area of the space labeled 'A103'?\", # Uses property lookup via ifc:hasPropertySet\n",
        "    \"Does a wall with label 'Basic Wall:Interior - Furring (152 mm Stud):190774' exist?\" # Should generate an ASK query\n",
        "]\n",
        "\n",
        "# Query using GraphRAG if initialized\n",
        "# Query using QueryRAG if initialized\n",
        "if query_rag and query_rag.chain:\n",
        "    for question in graph_questions:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Question: {question}\")\n",
        "        print(\"=\"*50)\n",
        "        \n",
        "        try:\n",
        "            result = query_rag.query(question)\n",
        "            \n",
        "            print(\"\\nGenerated SPARQL:\")\n",
        "            print(result[\"sparql_query\"] or \"N/A\")\n",
        "            \n",
        "            # Display Raw Results (nicer formatting)\n",
        "            print(\"\\nRaw Results:\")\n",
        "            if isinstance(result[\"raw_results\"] , list):\n",
        "                if not result[\"raw_results\"]:\n",
        "                    print(\"[]\")\n",
        "                else:\n",
        "                    print(\"[\")\n",
        "                    for i, res_dict in enumerate(result[\"raw_results\"]):\n",
        "                        if i >= 5: # Limit display for brevity\n",
        "                             print(f\"  ... ({len(result['raw_results']) - 5} more)\")\n",
        "                             break\n",
        "                        print(f\"  {res_dict}\")\n",
        "                    print(\"]\")\n",
        "            else:\n",
        "                print(result[\"raw_results\"] or \"N/A\")\n",
        "            \n",
        "            print(\"\\nFormatted Answer:\")\n",
        "            print(result[\"answer\"])\n",
        "            \n",
        "        except Exception as e:\n",
        "             print(f\"\\n❌ An unexpected error occurred during query processing: {e}\")\n",
        "             # Consider adding traceback here for debugging\n",
        "             # import traceback\n",
        "             # traceback.print_exc()\n",
        "else:\n",
        "    print(\"QueryRAG was not initialized successfully (check API key and file paths), skipping queries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Understanding the RAG Pipeline(s)\n",
        "\n",
        "Let's break down how the RAG systems work:\n",
        "\n",
        "**Vector-Based RAG (Section 2 & 4):**\n",
        "1. **Document Loading**: Loads pre-computed *embeddings* from JSON files.\n",
        "2. **Vector Store Creation**: Embeddings loaded into FAISS for *semantic similarity search*.\n",
        "3. **Query Processing**: Question is embedded -> FAISS finds similar documents -> Context is retrieved.\n",
        "4. **Answer Generation**: LLM uses question + retrieved *text context* to generate answer.\n",
        "5. **Source Tracking**: Tracks *source documents* used for context.\n",
        "\n",
        "**Graph-Based RAG (Section 3):**\n",
        "1. **Graph Loading**: Loads *RDF triples* from TTL files into a graph store (`pyoxigraph`).\n",
        "2. **NL-to-SPARQL**: LLM translates natural language question into a *SPARQL query* using schema/examples.\n",
        "3. **Query Execution**: The generated SPARQL query is executed directly against the graph database.\n",
        "4. **Answer Formatting**: Results from the SPARQL query are formatted (potentially using LLM again in future) into a readable answer.\n",
        "5. **Transparency**: Shows the *generated SPARQL query* and *raw graph results*.\n",
        "\n",
        "These approaches allow models to answer questions based on specific building information, either by finding relevant text snippets (vector) or querying structured data relationships (graph)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Next Steps\n",
        "\n",
        "Here are some ways to extend these RAG systems:\n",
        "\n",
        "**Vector RAG:**\n",
        "1. Try different embedding models.\n",
        "2. Experiment with different LLMs for generation.\n",
        "3. Improve text chunking strategies before embedding.\n",
        "\n",
        "**Graph RAG:**\n",
        "4. Refine the NL-to-SPARQL prompt with more/better few-shot examples or improved schema representation.\n",
        "5. Use a more powerful LLM (like GPT-4) for better SPARQL generation accuracy.\n",
        "6. Implement validation for generated SPARQL queries before execution.\n",
        "7. Use an LLM to summarize the raw SPARQL results into a more natural answer.\n",
        "\n",
        "**Hybrid Approaches:**\n",
        "8. Combine vector search with graph queries (e.g., use vector search to find relevant entities, then use graph queries to get specific properties of those entities).\n",
        "9. Use graph context to enhance vector retrieval or LLM generation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
