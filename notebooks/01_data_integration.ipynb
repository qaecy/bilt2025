{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Integration for Building Information Modeling\n",
        "\n",
        "This notebook demonstrates the process of converting raw building information data (IFC files and PDF documents) into RDF graph format for further processing with Graph RAG (Retrieval Augmented Generation). Preparing our data for graph-based knowledge retrieval\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- Process IFC (Industry Foundation Classes) building models and PDF product documentation into semantic graphs (ttl)\n",
        "- Convert TTL files into embeddings\n",
        "- Analyze and visualize the results\n",
        "\n",
        "## 0. Setup\n",
        "This notebook can run in either Google Colab or locally. The setup cell below automatically configures your environment by detecting whether it's running in Colab or locally, cloning the repository if needed (Colab), and installing dependencies from `requirements.txt`.\n",
        "\n",
        "**Key Point:** This setup ensures the notebook runs consistently anywhere with minimal configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    from IPython import get_ipython\n",
        "    IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Configure environment\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/qaecy/bilt2025.git\n",
        "    %cd bilt2025\n",
        "    requirements_path = \"requirements.txt\"\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "else:    \n",
        "    # Find requirements.txt based on current directory\n",
        "    current_dir = Path().resolve()\n",
        "    requirements_path = \"../requirements.txt\" if current_dir.name == \"notebooks\" else \"requirements.txt\"\n",
        "    print(f\"Looking for requirements at: {Path(requirements_path).resolve()}\")\n",
        "\n",
        "# Install dependencies if requirements.txt exists\n",
        "if os.path.exists(requirements_path):\n",
        "    %pip install -r {requirements_path}\n",
        "    if IN_COLAB:\n",
        "        %pip install -e .\n",
        "    print(\"✓ Environment setup complete\")\n",
        "else:\n",
        "    print(\"⚠️ Could not find requirements.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "All required packages are listed in `requirements.txt` and installed automatically by the setup cell above. Key libraries include:\n",
        "- **Data Processing:** pandas, rdflib\n",
        "- **IFC Processing:** ifcopenshell\n",
        "- **PDF Processing:** pymupdf\n",
        "- **Embedding:** openai-sdk\n",
        "\n",
        "### Data Flow Overview\n",
        "The diagram below illustrates the process: Input files (IFC/PDF) are processed by converters into RDF/Turtle format, then transformed into embeddings for analysis and use in the next Graph RAG lab.\n",
        "\n",
        "```\n",
        "┌───────────────┐     ┌─────────────────┐     ┌────────────────┐     ┌──────────────────┐     ┌─────────────────┐\n",
        "│  Input Data   │     │   Converters    │     │  Output Data   │     │    Embedding     │     │     Analysis    │\n",
        "├───────────────┤ --> ├─────────────────┤ --> ├────────────────┤ --> ├──────────────────┤ --> ├─────────────────┤\n",
        "│  IFC Files    │     │ ifc_converter   │     │      RDF       │     │ ttl_to_embedding │     │ Triple Count    │\n",
        "│  PDF Files    │     │ pdf_converter   │     │  (Turtle fmt)  │     │                  │     │ Size Comparison │\n",
        "└───────────────┘     └─────────────────┘     └────────────────┘     └──────────────────┘     └─────────────────┘\n",
        "                                                                                                       ↓\n",
        "                                                                                       ┌─────────────────────────┐\n",
        "                                                                                       │  Graph RAG (Next Lab)   │\n",
        "                                                                                       └─────────────────────────┘\n",
        "```\n",
        "\n",
        "## 1. Import Libraries\n",
        "\n",
        "Here we import the necessary libraries. Note the different handling for Colab vs. local environments to ensure correct path setup.\n",
        "\n",
        "**Key Point:** These libraries provide the tools to process our building data files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from rdflib import Graph\n",
        "\n",
        "# Add project to path if running locally\n",
        "if not IN_COLAB:\n",
        "    project_root = Path().resolve()\n",
        "    if project_root.name == 'notebooks':\n",
        "        project_root = project_root.parent\n",
        "    if str(project_root) not in sys.path:\n",
        "        sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import converters\n",
        "from src.graph_converter.ifc_converter import ifc_to_ttl\n",
        "from src.graph_converter.pdf_converter import pdf_to_ttl\n",
        "from src.graph_converter.embedding_converter import ttl_to_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Data Paths\n",
        "\n",
        "Next, we'll set up the paths for our raw data and output directories. This uses a consistent approach to find paths relative to the project root, whether running locally or in Colab.\n",
        "\n",
        "**Key Point:** Paths are consisten and matches our expectation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths with consistent approach for Colab and local environments\n",
        "project_root = Path().resolve()\n",
        "if project_root.name == 'notebooks':\n",
        "    project_root = project_root.parent\n",
        "\n",
        "DATA_DIR = project_root / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\" / \"buildingsmart_duplex\"\n",
        "GRAPH_DIR = DATA_DIR / \"graph\" / \"buildingsmart_duplex\"\n",
        "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\" / \"buildingsmart_duplex\"\n",
        "\n",
        "# Ensure directories exist\n",
        "for dir_path in [RAW_DIR, GRAPH_DIR, EMBEDDINGS_DIR]:\n",
        "    dir_path.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Raw data: {RAW_DIR}\")\n",
        "print(f\"Output directory: {GRAPH_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. List Available Data Files\n",
        "\n",
        "Let's examine the available data files. IFC files contain 3D building models and associated data, while PDF files typically contain product specifications or documentation.\n",
        "\n",
        "**Key Point:** We have two complementary data sources: IFC files for structured building geometric and properties data, and PDF files for unstructured textual specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List and display available files\n",
        "ifc_files = list(RAW_DIR.glob(\"*.ifc\"))\n",
        "pdf_files = list(RAW_DIR.glob(\"*.pdf\"))\n",
        "\n",
        "# Create DataFrame with file information\n",
        "file_info = [{\n",
        "    \"Filename\": file.name,\n",
        "    \"Type\": file.suffix[1:].upper(),\n",
        "    \"Size (MB)\": round(file.stat().st_size / (1024 * 1024), 2)\n",
        "} for file in ifc_files + pdf_files]\n",
        "\n",
        "file_df = pd.DataFrame(file_info).sort_values([\"Type\", \"Size (MB)\"], ascending=[True, False])\n",
        "display(file_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Processing IFC Files\n",
        "\n",
        "We'll define a unified function `process_files` to use in conversion of both IFC, PDF and embedding files which includes caching (`force_reprocess=False` by default) to avoid redundant processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_files(files, converter_func, output_dir, output_format, force_reprocess=False):\n",
        "    \"\"\"Process files to TTL format with caching\n",
        "    \n",
        "    Args:\n",
        "        files: List of files to process\n",
        "        converter_func: Function to convert files to TTL\n",
        "        output_dir: Directory to save TTL files\n",
        "        force_reprocess: Whether to regenerate existing TTL files\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for file in files:\n",
        "        output_file = output_dir / f\"{file.stem}.{output_format}\"\n",
        "        \n",
        "        # Use cache if file exists and not forced to reprocess\n",
        "        if output_file.exists() and not force_reprocess:\n",
        "            print(f\"✓ Using cached {file.name}\")\n",
        "            results.append({\n",
        "                \"Filename\": file.name,\n",
        "                \"Status\": \"Cached\",\n",
        "                \"Processing Time\": \"N/A\",\n",
        "                \"Output Size (MB)\": round(output_file.stat().st_size / (1024 * 1024), 2)\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"Processing {file.name}...\")\n",
        "            start_time = time.time()\n",
        "            converter_func(str(file), str(output_file))            \n",
        "            processing_time = round(time.time() - start_time, 2)\n",
        "            print(f\"✓ Processed {file.name} in {processing_time} seconds\")\n",
        "            \n",
        "            results.append({\n",
        "                \"Filename\": file.name,\n",
        "                \"Status\": \"Success\",\n",
        "                \"Processing Time\": f\"{processing_time} sec\",\n",
        "                \"Output Size (MB)\": round(output_file.stat().st_size / (1024 * 1024), 2)\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file.name}: {str(e)}\")\n",
        "            results.append({\n",
        "                \"Filename\": file.name,\n",
        "                \"Status\": f\"Error: {str(e)[:50]}...\",\n",
        "                \"Processing Time\": \"N/A\",\n",
        "                \"Output Size (MB)\": \"N/A\"\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For IFC files, this function utilizes the `ifc_to_ttl` converter which focuses on extracting structured semantic data from the IFC model.\n",
        "\n",
        "\n",
        "**Behind the Scenes (`ifc_to_ttl`):**\n",
        "- Parses the `.ifc` file using `ifcopenshell`.\n",
        "- Maps IFC entities (e.g., `IfcWall`, `IfcSpace`) and their relationships to RDF triples, using standard ontologies like BOT (Building Ontology) and aligning with IFC-OWL where possible.\n",
        "- Extracts Property Sets (`IfcPropertySet`) and their individual properties (e.g., `IfcPropertySingleValue`), converting values (text, numbers, booleans) into appropriate RDF literals.\n",
        "- Attempts to include unit information (e.g., using QUDT ontology mappings) for relevant properties.\n",
        "- Adds provenance details (source file, tool version, timestamp).\n",
        "- Validates the resulting graph against SHACL shapes (`src/graph_converter/ifc_converter_shapes.ttl`) before saving the `.ttl` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ifc_results = process_files(ifc_files, ifc_to_ttl, GRAPH_DIR, \"ttl\", force_reprocess=False)\n",
        "display(ifc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Processing PDF Documents\n",
        "\n",
        "Similarly, we'll process PDF files using the same `process_files` function. This time, it internally calls the `pdf_to_ttl` converter, which handles PDF-specific extraction and conversion.\n",
        "\n",
        "**Behind the Scenes (`pdf_to_ttl`):**\n",
        "- Opens the PDF using `PyMuPDF` (fitz).\n",
        "- Extracts document metadata (title, author, etc.) and page-level details (dimensions, rotation).\n",
        "- Extracts text content block by block, retaining basic formatting metadata (font, size, color, position).\n",
        "- Chunks the extracted text from each page into smaller, semantically coherent segments using a sentence splitter (`wtpsplit_lite`).\n",
        "- Represents the document structure (document → page → chunk) and content in RDF using standard vocabularies like FABIO (document types), DCTERMS (metadata), and CNT (text content `cnt:chars`).\n",
        "- Adds formatting and positional metadata using a custom `pdo:` namespace.\n",
        "- Validates the resulting graph against SHACL shapes (`src/graph_converter/pdf_converter_shapes.ttl`) before saving.\n",
        "\n",
        "**Key Point:** The `pdf_to_ttl` converter focuses on extracting and structuring textual information from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_results = process_files(pdf_files, pdf_to_ttl, GRAPH_DIR, \"ttl\", force_reprocess=False)\n",
        "display(pdf_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analysis of Processed Data\n",
        "\n",
        "Let's analyze the resulting RDF graphs (stored as TTL files). We'll compare metrics like file size and triple count between the data derived from IFC and PDF sources using the `analyze_graph` helper function.\n",
        "\n",
        "**Key Point:** This analysis quantifies the richness of data extracted. Notice how IFC files typically yield significantly more structured triples compared to PDFs, as reflected in the IFC:PDF ratio table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze processed files\n",
        "ttl_files = list(GRAPH_DIR.glob(\"*.ttl\"))\n",
        "\n",
        "# Function to safely load and analyze a graph file\n",
        "def analyze_graph(ttl_file, ifc_stems):\n",
        "    try:\n",
        "        g = Graph()\n",
        "        g.parse(str(ttl_file), format=\"turtle\")\n",
        "        source_type = \"IFC\" if ttl_file.stem in ifc_stems else \"PDF\"\n",
        "        return {\n",
        "            \"Filename\": ttl_file.name,\n",
        "            \"Source Type\": source_type,\n",
        "            \"Size (MB)\": round(ttl_file.stat().st_size / (1024 * 1024), 2),\n",
        "            \"Triple Count\": len(g)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing {ttl_file.name}: {str(e)}\")\n",
        "        return {\n",
        "            \"Filename\": ttl_file.name,\n",
        "            \"Source Type\": \"Unknown\",\n",
        "            \"Size (MB)\": round(ttl_file.stat().st_size / (1024 * 1024), 2),\n",
        "            \"Triple Count\": 0\n",
        "        }\n",
        "\n",
        "# Get list of IFC file stems for source type detection\n",
        "ifc_stems = [ifc.stem for ifc in ifc_files]\n",
        "\n",
        "# Gather information about all TTL files\n",
        "graph_info = [analyze_graph(ttl_file, ifc_stems) for ttl_file in ttl_files]\n",
        "graph_df = pd.DataFrame(graph_info).sort_values([\"Source Type\", \"Triple Count\"], ascending=[True, False])\n",
        "display(graph_df)\n",
        "\n",
        "# Visualize comparison between IFC and PDF data\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Size comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "graphs_by_source = graph_df.groupby(\"Source Type\")[\"Size (MB)\"].sum()\n",
        "graphs_by_source.plot(kind=\"bar\", color=[\"#2196F3\", \"#FF9800\"])\n",
        "plt.title(\"Total Size by Source Type\")\n",
        "plt.ylabel(\"Size (MB)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Triple count comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "triples_by_source = graph_df.groupby(\"Source Type\")[\"Triple Count\"].sum()\n",
        "triples_by_source.plot(kind=\"bar\", color=[\"#2196F3\", \"#FF9800\"])\n",
        "plt.title(\"Total Triples by Source Type\")\n",
        "plt.ylabel(\"Number of Triples\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create ratio comparison DataFrame\n",
        "try:\n",
        "    # Calculate size and triple ratios\n",
        "    size_by_type = graphs_by_source.to_dict()\n",
        "    triples_by_type = triples_by_source.to_dict()\n",
        "    \n",
        "    # Only calculate ratios if both IFC and PDF data exist\n",
        "    if 'IFC' in size_by_type and 'PDF' in size_by_type and size_by_type['PDF'] > 0:\n",
        "        size_ratio = f\"{size_by_type['IFC'] / size_by_type['PDF']:.2f}:1\"\n",
        "    else:\n",
        "        size_ratio = \"N/A\"\n",
        "        \n",
        "    if 'IFC' in triples_by_type and 'PDF' in triples_by_type and triples_by_type['PDF'] > 0:\n",
        "        triple_ratio = f\"{triples_by_type['IFC'] / triples_by_type['PDF']:.2f}:1\"\n",
        "    else:\n",
        "        triple_ratio = \"N/A\"\n",
        "    \n",
        "    # Create ratio comparison DataFrame\n",
        "    ratio_data = {\n",
        "        'Metric': ['Size (MB)', 'Triple Count'],\n",
        "        'IFC Total': [size_by_type.get('IFC', 0), triples_by_type.get('IFC', 0)],\n",
        "        'PDF Total': [size_by_type.get('PDF', 0), triples_by_type.get('PDF', 0)],\n",
        "        'Ratio (IFC:PDF)': [size_ratio, triple_ratio]\n",
        "    }\n",
        "    \n",
        "    display(pd.DataFrame(ratio_data))\n",
        "    print(\"This comparison shows how IFC files typically generate much richer semantic data compared to PDF documents.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error calculating ratios: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sample Triple Analysis\n",
        "\n",
        "Let's examine sample triples (Subject-Predicate-Object statements) to understand the structure of our semantic graphs. This helps visualize the relationships extracted from the source files.\n",
        "\n",
        "**Key Point:** These triples form the atomic units of our knowledge graph, representing facts and relationships about the building components and documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sample_triples(file_path, sample_size=10):\n",
        "    \"\"\"Display sample triples and namespace information from an RDF file\"\"\"\n",
        "    g = Graph()\n",
        "    g.parse(str(file_path), format=\"turtle\")\n",
        "    \n",
        "    print(f\"Analyzing {file_path.name} - Total triples: {len(g)}\")\n",
        "    \n",
        "    # Show sample triples\n",
        "    print(f\"Sample triples:\")\n",
        "    for i, (s, p, o) in enumerate(list(g)[:sample_size]):\n",
        "        print(f\"  {i+1}. {s} → {p} → {o}\")\n",
        "        \n",
        "# Sample one IFC and one PDF file\n",
        "ifc_sample = next((f for f in ttl_files if f.stem in ifc_stems), None)\n",
        "pdf_sample = next((f for f in ttl_files if f.stem not in ifc_stems), None)\n",
        "\n",
        "if ifc_sample:\n",
        "    analyze_sample_triples(ifc_sample)\n",
        "    print(\"\" + \"-\"*80 + \"\")\n",
        "    \n",
        "if pdf_sample:\n",
        "    analyze_sample_triples(pdf_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Entity-Centric Embedding Generation\n",
        "\n",
        "To prepare our knowledge graph for semantic search and Retrieval-Augmented Generation (RAG), we'll generate *entity-centric* embeddings using the `ttl_to_embeddings` converter. This involves:\n",
        "1. Grouping all triples by their subject (the entity). Each entity might be a building element (like a wall or door from IFC) or a document chunk (from PDF).\n",
        "2. For each entity, creating a single text representation by concatenating its type, label, and relevant properties (selected based on a predefined list `ALLOWED_PREDICATES` in the code).\n",
        "   - For IFC entities, this process includes traversing into linked Property Sets (`IfcPropertySet`) to gather related properties, providing richer context.\n",
        "   - For PDF text chunks (`cnt:ContentAsText`), only the actual text content (`cnt:chars`) is typically used.\n",
        "3. Using a pre-trained language model (specifically, OpenAI's `text-embedding-3-small` by default) to generate a vector embedding for this consolidated entity text.\n",
        "4. Saving the entity URI, its readable name, the generated text, and the embedding vector into a JSON file.\n",
        "\n",
        "This approach creates comprehensive vector representations capturing the semantic context around each entity.\n",
        "\n",
        "**Key Point:** These embeddings enable semantic similarity searches over building entities and document chunks, which is crucial for the Graph RAG system in the next lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_results = process_files(ttl_files, ttl_to_embeddings, EMBEDDINGS_DIR, \"json\", force_reprocess=False)\n",
        "display(embedding_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "We have successfully converted raw IFC and PDF building data into structured RDF graphs and generated entity-centric embeddings. This prepares the data for graph-based knowledge retrieval.\n",
        "\n",
        "**Key Point:** We've transformed diverse building data into a unified, queryable format ready for Graph RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary statistics\n",
        "embedding_files = list(EMBEDDINGS_DIR.glob(\"*.json\"))\n",
        "total_raw_size = sum(f.stat().st_size for f in (ifc_files + pdf_files)) / (1024 * 1024)\n",
        "total_graph_size = sum(f.stat().st_size for f in ttl_files) / (1024 * 1024)\n",
        "total_embedding_size = sum(f.stat().st_size for f in embedding_files if 'embedding_files' in locals()) / (1024 * 1024) if 'embedding_files' in locals() else 0\n",
        "\n",
        "summary_data = {\n",
        "    'Metric': [\n",
        "        'Total files processed',\n",
        "        'IFC files processed',\n",
        "        'PDF files processed',\n",
        "        'Total RDF files created',\n",
        "        'Total embedding files created',\n",
        "        'Total raw data size (MB)',\n",
        "        'Total RDF graph size (MB)',\n",
        "        'Total embeddings size (MB)'\n",
        "    ],\n",
        "    'Value': [\n",
        "        len(ifc_files) + len(pdf_files),\n",
        "        len(ifc_files),\n",
        "        len(pdf_files),\n",
        "        len(ttl_files),\n",
        "        len(embedding_files) if 'embedding_files' in locals() else 0,\n",
        "        round(total_raw_size, 2),\n",
        "        round(total_graph_size, 2),\n",
        "        round(total_embedding_size, 2)\n",
        "    ]\n",
        "}\n",
        "\n",
        "display(pd.DataFrame(summary_data))\n",
        "print(\"The processed data is now ready for Graph RAG in the next lab session.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Troubleshooting\n",
        "\n",
        "If you encounter issues while running this notebook, here are common problems and solutions:\n",
        "\n",
        "### Missing Dependencies\n",
        "- **Problem**: Import errors like `ModuleNotFoundError`.\n",
        "- **Solution**: Ensure the setup cell (Section 0) ran successfully and installed packages from `requirements.txt`. If running locally outside the standard project structure, ensure `requirements.txt` is found. You might need to run `%pip install -r path/to/requirements.txt` manually.\n",
        "\n",
        "### Processing Errors (Memory)\n",
        "- **Problem**: Large IFC files cause out-of-memory errors during conversion.\n",
        "- **Solution**: Restart the kernel and try processing smaller files first. Ensure your machine or Colab instance has sufficient RAM (at least 8GB recommended, more for larger files). Close other memory-intensive applications.\n",
        "\n",
        "### Embedding Generation Errors\n",
        "- **Problem**: OPENAI_API_KEY not set.\n",
        "- **Solution**: Follow instructions in readme.\n",
        "\n",
        "### File Not Found Errors\n",
        "- **Problem**: Input data files (`.ifc`, `.pdf`) or output directories are not found.\n",
        "- **Solution**: Double-check the directory structure defined in Section 2 (`Define Data Paths`). Ensure the `data/raw/buildingsmart_duplex/` directory contains the necessary source files relative to your project root. The expected structure is:\n",
        "  ```\n",
        "  project_root/\n",
        "  ├── data/\n",
        "  │   ├── raw/buildingsmart_duplex/  (Input IFC & PDF files)\n",
        "  │   ├── graph/buildingsmart_duplex/ (Output TTL files)\n",
        "  │   └── embeddings/buildingsmart_duplex/ (Output JSON embedding files)\n",
        "  ├── notebooks/\n",
        "  │   └── 01_data_integration.ipynb\n",
        "  ├── src/\n",
        "  └── requirements.txt\n",
        "  ```\n",
        "\n",
        "## Next Steps: Graph RAG\n",
        "\n",
        "The processed RDF graphs and embeddings created in this notebook form the foundation for the next lab session on Graph RAG. In that session, we will:\n",
        "1. Do a vector based RAG with our embeddings generated here.\n",
        "2. Load the TTL files into a unified knowledge graph database and use this in a query based RAG."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
