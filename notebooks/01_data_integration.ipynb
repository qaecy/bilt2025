{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Integration for Building Information Modeling\n",
        "\n",
        "This notebook demonstrates the process of converting raw building information data (IFC files and PDF documents) into RDF graph format for further processing with Graph RAG (Retrieval Augmented Generation).\n",
        "\n",
        "**What we'll accomplish:**\n",
        "- Process IFC (Industry Foundation Classes) building models into semantic graphs\n",
        "- Convert PDF product documentation into linked data format\n",
        "- Analyze and visualize the results\n",
        "- Prepare the data for graph-based knowledge retrieval\n",
        "\n",
        "## 0. Setup\n",
        "This notebook can run in either Google Colab or locally. The setup cell will automatically configure your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "try:\n",
        "    from IPython import get_ipython\n",
        "    IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Configure environment\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/qaecy/bilt2025.git\n",
        "    %cd bilt2025\n",
        "    requirements_path = \"requirements.txt\"\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "else:    \n",
        "    # Find requirements.txt based on current directory\n",
        "    current_dir = Path().resolve()\n",
        "    requirements_path = \"../requirements.txt\" if current_dir.name == \"notebooks\" else \"requirements.txt\"\n",
        "    print(f\"Looking for requirements at: {Path(requirements_path).resolve()}\")\n",
        "\n",
        "# Install dependencies if requirements.txt exists\n",
        "if os.path.exists(requirements_path):\n",
        "    %pip install -r {requirements_path}\n",
        "    if IN_COLAB:\n",
        "        %pip install -e .\n",
        "    print(\"✓ Environment setup complete\")\n",
        "else:\n",
        "    print(\"⚠️ Could not find requirements.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "This notebook automatically installs all required packages from `requirements.txt` when running the setup cell above. The key libraries include data processing tools (pandas, rdflib), visualization tools (matplotlib), and domain-specific processors for IFC (ifcopenshell) and PDF (pymupdf) files.\n",
        "\n",
        "# TODO update flow as we also have ttl to embedding\n",
        "\n",
        "## Data Flow Overview\n",
        "\n",
        "```\n",
        "┌───────────────┐     ┌─────────────────┐     ┌────────────────┐     ┌─────────────────┐\n",
        "│  Input Data   │     │   Converters    │     │  Output Data   │     │     Analysis    │\n",
        "├───────────────┤ --> ├─────────────────┤ --> ├────────────────┤ --> ├─────────────────┤\n",
        "│  IFC Files    │     │ ifc_converter   │     │      RDF       │     │ Triple Count    │\n",
        "│  PDF Files    │     │ pdf_converter   │     │  (Turtle fmt)  │     │ Size Comparison │\n",
        "└───────────────┘     └─────────────────┘     └────────────────┘     └─────────────────┘\n",
        "                                                                              ↓\n",
        "                                                             ┌─────────────────────────┐\n",
        "                                                             │  Graph RAG (Next Lab)   │\n",
        "                                                             └─────────────────────────┘\n",
        "```\n",
        "\n",
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from rdflib import Graph\n",
        "\n",
        "# Add project to path if running locally\n",
        "if not IN_COLAB:\n",
        "    project_root = Path().resolve()\n",
        "    if project_root.name == 'notebooks':\n",
        "        project_root = project_root.parent\n",
        "    if str(project_root) not in sys.path:\n",
        "        sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import converters\n",
        "from src.graph_converter.ifc_converter import ifc_to_ttl\n",
        "from src.graph_converter.pdf_converter import pdf_to_ttl\n",
        "from src.graph_converter.embedding_converter import ttl_to_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Define Data Paths\n",
        "\n",
        "Next, we'll set up the paths for our raw data and output directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define paths with consistent approach for Colab and local environments\n",
        "project_root = Path().resolve()\n",
        "if project_root.name == 'notebooks':\n",
        "    project_root = project_root.parent\n",
        "\n",
        "DATA_DIR = project_root / \"data\"\n",
        "RAW_DIR = DATA_DIR / \"raw\" / \"buildingsmart_duplex\"\n",
        "GRAPH_DIR = DATA_DIR / \"graph\" / \"buildingsmart_duplex\"\n",
        "EMBEDDINGS_DIR = DATA_DIR / \"embeddings\" / \"buildingsmart_duplex\"\n",
        "\n",
        "# Ensure directories exist\n",
        "for dir_path in [RAW_DIR, GRAPH_DIR, EMBEDDINGS_DIR]:\n",
        "    dir_path.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Raw data: {RAW_DIR}\\nOutput directory: {GRAPH_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. List Available Data Files\n",
        "\n",
        "Let's examine the available data files - IFC files contain 3D building models, while PDF files contain product specifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List and display available files\n",
        "ifc_files = list(RAW_DIR.glob(\"*.ifc\"))\n",
        "pdf_files = list(RAW_DIR.glob(\"*.pdf\"))\n",
        "\n",
        "# Create DataFrame with file information\n",
        "file_info = [{\n",
        "    \"Filename\": file.name,\n",
        "    \"Type\": file.suffix[1:].upper(),\n",
        "    \"Size (MB)\": round(file.stat().st_size / (1024 * 1024), 2)\n",
        "} for file in ifc_files + pdf_files]\n",
        "\n",
        "file_df = pd.DataFrame(file_info).sort_values([\"Type\", \"Size (MB)\"], ascending=[True, False])\n",
        "display(file_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Processing IFC Files\n",
        "\n",
        "We'll convert IFC files to RDF/Turtle format using a converter that maps building elements to a semantic graph. The function includes caching to avoid redundant processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_files(files, converter_func, output_dir, output_format, force_reprocess=False):\n",
        "    \"\"\"Process files to TTL format with caching\n",
        "    \n",
        "    Args:\n",
        "        files: List of files to process\n",
        "        converter_func: Function to convert files to TTL\n",
        "        output_dir: Directory to save TTL files\n",
        "        force_reprocess: Whether to regenerate existing TTL files\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for file in files:\n",
        "        output_file = output_dir / f\"{file.stem}.{output_format}\"\n",
        "        \n",
        "        # Use cache if file exists and not forced to reprocess\n",
        "        if output_file.exists() and not force_reprocess:\n",
        "            print(f\"✓ Using cached {file.name}\")\n",
        "            results.append({\n",
        "                \"Filename\": file.name,\n",
        "                \"Status\": \"Cached\",\n",
        "                \"Processing Time\": \"N/A\",\n",
        "                \"Output Size (MB)\": round(output_file.stat().st_size / (1024 * 1024), 2)\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(f\"Processing {file.name}...\")\n",
        "            start_time = time.time()\n",
        "            converter_func(str(file), str(output_file))            \n",
        "            processing_time = round(time.time() - start_time, 2)\n",
        "            print(f\"✓ Processed {file.name} in {processing_time} seconds\")\n",
        "            \n",
        "            results.append({\n",
        "                \"Filename\": file.name,\n",
        "                \"Status\": \"Success\",\n",
        "                \"Processing Time\": f\"{processing_time} sec\",\n",
        "                \"Output Size (MB)\": round(output_file.stat().st_size / (1024 * 1024), 2)\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing {file.name}: {str(e)}\")\n",
        "            results.append({\n",
        "                \"Filename\": file.name,\n",
        "                \"Status\": f\"Error: {str(e)[:50]}...\",\n",
        "                \"Processing Time\": \"N/A\",\n",
        "                \"Output Size (MB)\": \"N/A\"\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Process IFC files\n",
        "ifc_results = process_files(ifc_files, ifc_to_ttl, GRAPH_DIR, \"ttl\", force_reprocess=False)\n",
        "display(ifc_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Processing PDF Documents\n",
        "\n",
        "Similarly, we'll process PDF files that contain product specifications to complement the geometric data in IFC files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process PDF files using the same utility function\n",
        "pdf_results = process_files(pdf_files, pdf_to_ttl, GRAPH_DIR, \"ttl\", force_reprocess=False)\n",
        "display(pdf_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analysis of Processed Data\n",
        "\n",
        "Let's analyze the resulting RDF graphs, comparing metrics between IFC and PDF sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze processed files\n",
        "ttl_files = list(GRAPH_DIR.glob(\"*.ttl\"))\n",
        "\n",
        "# Function to safely load and analyze a graph file\n",
        "def analyze_graph(ttl_file, ifc_stems):\n",
        "    try:\n",
        "        g = Graph()\n",
        "        g.parse(str(ttl_file), format=\"turtle\")\n",
        "        source_type = \"IFC\" if ttl_file.stem in ifc_stems else \"PDF\"\n",
        "        return {\n",
        "            \"Filename\": ttl_file.name,\n",
        "            \"Source Type\": source_type,\n",
        "            \"Size (MB)\": round(ttl_file.stat().st_size / (1024 * 1024), 2),\n",
        "            \"Triple Count\": len(g)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing {ttl_file.name}: {str(e)}\")\n",
        "        return {\n",
        "            \"Filename\": ttl_file.name,\n",
        "            \"Source Type\": \"Unknown\",\n",
        "            \"Size (MB)\": round(ttl_file.stat().st_size / (1024 * 1024), 2),\n",
        "            \"Triple Count\": 0\n",
        "        }\n",
        "\n",
        "# Get list of IFC file stems for source type detection\n",
        "ifc_stems = [ifc.stem for ifc in ifc_files]\n",
        "\n",
        "# Gather information about all TTL files\n",
        "graph_info = [analyze_graph(ttl_file, ifc_stems) for ttl_file in ttl_files]\n",
        "graph_df = pd.DataFrame(graph_info).sort_values([\"Source Type\", \"Triple Count\"], ascending=[True, False])\n",
        "display(graph_df)\n",
        "\n",
        "# Visualize comparison between IFC and PDF data\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Size comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "graphs_by_source = graph_df.groupby(\"Source Type\")[\"Size (MB)\"].sum()\n",
        "graphs_by_source.plot(kind=\"bar\", color=[\"#2196F3\", \"#FF9800\"])\n",
        "plt.title(\"Total Size by Source Type\")\n",
        "plt.ylabel(\"Size (MB)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Triple count comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "triples_by_source = graph_df.groupby(\"Source Type\")[\"Triple Count\"].sum()\n",
        "triples_by_source.plot(kind=\"bar\", color=[\"#2196F3\", \"#FF9800\"])\n",
        "plt.title(\"Total Triples by Source Type\")\n",
        "plt.ylabel(\"Number of Triples\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create ratio comparison DataFrame\n",
        "try:\n",
        "    # Calculate size and triple ratios\n",
        "    size_by_type = graphs_by_source.to_dict()\n",
        "    triples_by_type = triples_by_source.to_dict()\n",
        "    \n",
        "    # Only calculate ratios if both IFC and PDF data exist\n",
        "    if 'IFC' in size_by_type and 'PDF' in size_by_type and size_by_type['PDF'] > 0:\n",
        "        size_ratio = f\"{size_by_type['IFC'] / size_by_type['PDF']:.2f}:1\"\n",
        "    else:\n",
        "        size_ratio = \"N/A\"\n",
        "        \n",
        "    if 'IFC' in triples_by_type and 'PDF' in triples_by_type and triples_by_type['PDF'] > 0:\n",
        "        triple_ratio = f\"{triples_by_type['IFC'] / triples_by_type['PDF']:.2f}:1\"\n",
        "    else:\n",
        "        triple_ratio = \"N/A\"\n",
        "    \n",
        "    # Create ratio comparison DataFrame\n",
        "    ratio_data = {\n",
        "        'Metric': ['Size (MB)', 'Triple Count'],\n",
        "        'IFC Total': [size_by_type.get('IFC', 0), triples_by_type.get('IFC', 0)],\n",
        "        'PDF Total': [size_by_type.get('PDF', 0), triples_by_type.get('PDF', 0)],\n",
        "        'Ratio (IFC:PDF)': [size_ratio, triple_ratio]\n",
        "    }\n",
        "    \n",
        "    display(pd.DataFrame(ratio_data))\n",
        "    print(\"This comparison shows how IFC files typically generate much richer semantic data compared to PDF documents.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error calculating ratios: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Sample Triple Analysis\n",
        "\n",
        "Let's examine sample triples to understand the structure of our semantic graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_sample_triples(file_path, sample_size=10):\n",
        "    \"\"\"Display sample triples and namespace information from an RDF file\"\"\"\n",
        "    g = Graph()\n",
        "    g.parse(str(file_path), format=\"turtle\")\n",
        "    \n",
        "    print(f\"Analyzing {file_path.name} - Total triples: {len(g)}\")\n",
        "    \n",
        "    # Show namespaces\n",
        "    print(\"\\nNamespaces:\")\n",
        "    for prefix, namespace in g.namespaces():\n",
        "        print(f\"  {prefix}: {namespace}\")\n",
        "    \n",
        "    # Show sample triples\n",
        "    print(f\"\\nSample triples:\")\n",
        "    for i, (s, p, o) in enumerate(list(g)[:sample_size]):\n",
        "        print(f\"  {i+1}. {s} → {p} → {o}\")\n",
        "        \n",
        "# Sample one IFC and one PDF file\n",
        "ifc_sample = next((f for f in ttl_files if f.stem in ifc_stems), None)\n",
        "pdf_sample = next((f for f in ttl_files if f.stem not in ifc_stems), None)\n",
        "\n",
        "if ifc_sample:\n",
        "    analyze_sample_triples(ifc_sample)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
        "    \n",
        "if pdf_sample:\n",
        "    analyze_sample_triples(pdf_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Entity-Centric Embedding Generation\n",
        "\n",
        "To prepare our knowledge graph for RAG, we'll generate entity-centric embeddings of the RDF files. This approach groups all triples with the same subject (entity) together, creating comprehensive representations of each entity for semantic search and retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_results = process_files(ttl_files, ttl_to_embeddings, EMBEDDINGS_DIR, \"json\", force_reprocess=False)\n",
        "display(embedding_results)\n",
        "embedding_files = list(EMBEDDINGS_DIR.glob(\"*.json\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "With our data converted to RDF format and embedded for semantic search, we're ready for the next step: graph-based retrieval augmented generation (Graph RAG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary statistics\n",
        "total_raw_size = sum(f.stat().st_size for f in (ifc_files + pdf_files)) / (1024 * 1024)\n",
        "total_graph_size = sum(f.stat().st_size for f in ttl_files) / (1024 * 1024)\n",
        "total_embedding_size = sum(f.stat().st_size for f in embedding_files if 'embedding_files' in locals()) / (1024 * 1024) if 'embedding_files' in locals() else 0\n",
        "\n",
        "summary_data = {\n",
        "    'Metric': [\n",
        "        'Total files processed',\n",
        "        'IFC files processed',\n",
        "        'PDF files processed',\n",
        "        'Total RDF files created',\n",
        "        'Total embedding files created',\n",
        "        'Total raw data size (MB)',\n",
        "        'Total RDF graph size (MB)',\n",
        "        'Total embeddings size (MB)'\n",
        "    ],\n",
        "    'Value': [\n",
        "        len(ifc_files) + len(pdf_files),\n",
        "        len(ifc_files),\n",
        "        len(pdf_files),\n",
        "        len(ttl_files),\n",
        "        len(embedding_files) if 'embedding_files' in locals() else 0,\n",
        "        round(total_raw_size, 2),\n",
        "        round(total_graph_size, 2),\n",
        "        round(total_embedding_size, 2)\n",
        "    ]\n",
        "}\n",
        "\n",
        "display(pd.DataFrame(summary_data))\n",
        "print(\"The processed data is now ready for Graph RAG in the next lab session.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Troubleshooting\n",
        "\n",
        "If you encounter issues while running this notebook, here are the most common problems and solutions:\n",
        "\n",
        "### Missing Dependencies\n",
        "- **Problem**: Import errors like `ModuleNotFoundError`\n",
        "- **Solution**: Check that the setup cell ran successfully. If not, run: `pip install -r requirements.txt`\n",
        "\n",
        "### Processing Errors\n",
        "- **Problem**: Large files cause out-of-memory errors\n",
        "- **Solution**: Restart kernel and process smaller files, or use a machine with more RAM\n",
        "\n",
        "### Embedding Generation\n",
        "- **Problem**: `sentence-transformers` not installed or errors during embedding\n",
        "- **Solution**: Run `pip install sentence-transformers` or increase RAM for large graphs\n",
        "\n",
        "### File Structure\n",
        "- **Problem**: Files not found\n",
        "- **Solution**: Ensure your directory structure matches:\n",
        "  ```\n",
        "  project_root/\n",
        "  ├── data/raw/buildingsmart_duplex/ (IFC & PDF files)\n",
        "  ├── data/graph/buildingsmart_duplex/ (output TTL files)\n",
        "  ├── data/embeddings/buildingsmart_duplex/ (output embedding files)\n",
        "  └── notebooks/01_data_integration.ipynb\n",
        "  ```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
